{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906d36da-768b-4b93-9cbe-4bb1086e3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "browsing_mobile_behaviour.csv    im_mobile_behaviour.csv\n",
      "column_descriptions.json         other_mobile_behaviour.csv\n",
      "fibre_behaviour.csv              streaming_mobile_behaviour.csv\n",
      "fibre_connected_devices_info.csv subscribers_info.csv\n",
      "file_access_mobile_behaviour.csv table_descriptions.json\n",
      "gaming_mobile_behaviour.csv      voip_mobile_behaviour.csv\n"
     ]
    }
   ],
   "source": [
    "!ls clean_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e794f2f-8bac-4d4e-9500-bb9eb4dbeaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for '_mobile_behaviour.csv' files in: ../data\n",
      "Found 7 files to process: gaming_mobile_behaviour.csv, streaming_mobile_behaviour.csv, im_mobile_behaviour.csv, voip_mobile_behaviour.csv, other_mobile_behaviour.csv, browsing_mobile_behaviour.csv, file_access_mobile_behaviour.csv\n",
      "\n",
      "--- Processing file: 'gaming_mobile_behaviour.csv' ---\n",
      "Successfully read 'gaming_mobile_behaviour.csv'. Original rows: 8671\n",
      "Column names converted to uppercase.\n",
      "Transforming 'DATE_ID' column...\n",
      "Sample of 'DATE_ID' before transformation:\n",
      "0    1/12/2024\n",
      "1    1/12/2024\n",
      "2    1/12/2024\n",
      "3    1/12/2024\n",
      "4    1/12/2024\n",
      "Name: DATE_ID, dtype: object\n",
      "Successfully transformed 'DATE_ID' column in 'gaming_mobile_behaviour.csv'.\n",
      "Sample of 'DATE_ID' after transformation:\n",
      "0    2024-12-01\n",
      "1    2024-12-01\n",
      "2    2024-12-01\n",
      "3    2024-12-01\n",
      "4    2024-12-01\n",
      "Name: DATE_ID, dtype: object\n",
      "Transformed file saved to: '../experiments/clean_data/gaming_mobile_behaviour.csv'.\n",
      "\n",
      "--- Processing file: 'streaming_mobile_behaviour.csv' ---\n",
      "Successfully read 'streaming_mobile_behaviour.csv'. Original rows: 85620\n",
      "Column names converted to uppercase.\n",
      "Transforming 'DATE_ID' column...\n",
      "Sample of 'DATE_ID' before transformation:\n",
      "0    7/12/2024\n",
      "1    7/12/2024\n",
      "2    2/12/2024\n",
      "3    4/12/2024\n",
      "4    6/12/2024\n",
      "Name: DATE_ID, dtype: object\n",
      "Successfully transformed 'DATE_ID' column in 'streaming_mobile_behaviour.csv'.\n",
      "Sample of 'DATE_ID' after transformation:\n",
      "0    2024-12-07\n",
      "1    2024-12-07\n",
      "2    2024-12-02\n",
      "3    2024-12-04\n",
      "4    2024-12-06\n",
      "Name: DATE_ID, dtype: object\n",
      "Transformed file saved to: '../experiments/clean_data/streaming_mobile_behaviour.csv'.\n",
      "\n",
      "--- Processing file: 'im_mobile_behaviour.csv' ---\n",
      "Successfully read 'im_mobile_behaviour.csv'. Original rows: 115801\n",
      "Column names converted to uppercase.\n",
      "Transforming 'DATE_ID' column...\n",
      "Sample of 'DATE_ID' before transformation:\n",
      "0    2/12/2024\n",
      "1    6/12/2024\n",
      "2    5/12/2024\n",
      "3    6/12/2024\n",
      "4    2/12/2024\n",
      "Name: DATE_ID, dtype: object\n",
      "Successfully transformed 'DATE_ID' column in 'im_mobile_behaviour.csv'.\n",
      "Sample of 'DATE_ID' after transformation:\n",
      "0    2024-12-02\n",
      "1    2024-12-06\n",
      "2    2024-12-05\n",
      "3    2024-12-06\n",
      "4    2024-12-02\n",
      "Name: DATE_ID, dtype: object\n",
      "Transformed file saved to: '../experiments/clean_data/im_mobile_behaviour.csv'.\n",
      "\n",
      "--- Processing file: 'voip_mobile_behaviour.csv' ---\n",
      "Successfully read 'voip_mobile_behaviour.csv'. Original rows: 11933\n",
      "Column names converted to uppercase.\n",
      "Transforming 'DATE_ID' column...\n",
      "Sample of 'DATE_ID' before transformation:\n",
      "0    4/12/2024\n",
      "1    5/12/2024\n",
      "2    5/12/2024\n",
      "3    4/12/2024\n",
      "4    5/12/2024\n",
      "Name: DATE_ID, dtype: object\n",
      "Successfully transformed 'DATE_ID' column in 'voip_mobile_behaviour.csv'.\n",
      "Sample of 'DATE_ID' after transformation:\n",
      "0    2024-12-04\n",
      "1    2024-12-05\n",
      "2    2024-12-05\n",
      "3    2024-12-04\n",
      "4    2024-12-05\n",
      "Name: DATE_ID, dtype: object\n",
      "Transformed file saved to: '../experiments/clean_data/voip_mobile_behaviour.csv'.\n",
      "\n",
      "--- Processing file: 'other_mobile_behaviour.csv' ---\n",
      "Successfully read 'other_mobile_behaviour.csv'. Original rows: 529832\n",
      "Column names converted to uppercase.\n",
      "Transforming 'DATE_ID' column...\n",
      "Sample of 'DATE_ID' before transformation:\n",
      "0    2/12/2024\n",
      "1    2/12/2024\n",
      "2    2/12/2024\n",
      "3    2/12/2024\n",
      "4    3/12/2024\n",
      "Name: DATE_ID, dtype: object\n",
      "Successfully transformed 'DATE_ID' column in 'other_mobile_behaviour.csv'.\n",
      "Sample of 'DATE_ID' after transformation:\n",
      "0    2024-12-02\n",
      "1    2024-12-02\n",
      "2    2024-12-02\n",
      "3    2024-12-02\n",
      "4    2024-12-03\n",
      "Name: DATE_ID, dtype: object\n",
      "Transformed file saved to: '../experiments/clean_data/other_mobile_behaviour.csv'.\n",
      "\n",
      "--- Processing file: 'browsing_mobile_behaviour.csv' ---\n",
      "Successfully read 'browsing_mobile_behaviour.csv'. Original rows: 652926\n",
      "Column names converted to uppercase.\n",
      "Transforming 'DATE_ID' column...\n",
      "Sample of 'DATE_ID' before transformation:\n",
      "0    2/12/2024\n",
      "1    2/12/2024\n",
      "2    2/12/2024\n",
      "3    5/12/2024\n",
      "4    2/12/2024\n",
      "Name: DATE_ID, dtype: object\n",
      "Successfully transformed 'DATE_ID' column in 'browsing_mobile_behaviour.csv'.\n",
      "Sample of 'DATE_ID' after transformation:\n",
      "0    2024-12-02\n",
      "1    2024-12-02\n",
      "2    2024-12-02\n",
      "3    2024-12-05\n",
      "4    2024-12-02\n",
      "Name: DATE_ID, dtype: object\n",
      "Transformed file saved to: '../experiments/clean_data/browsing_mobile_behaviour.csv'.\n",
      "\n",
      "--- Processing file: 'file_access_mobile_behaviour.csv' ---\n",
      "Successfully read 'file_access_mobile_behaviour.csv'. Original rows: 331669\n",
      "Column names converted to uppercase.\n",
      "Transforming 'DATE_ID' column...\n",
      "Sample of 'DATE_ID' before transformation:\n",
      "0    1/12/2024\n",
      "1    1/12/2024\n",
      "2    1/12/2024\n",
      "3    1/12/2024\n",
      "4    1/12/2024\n",
      "Name: DATE_ID, dtype: object\n",
      "Successfully transformed 'DATE_ID' column in 'file_access_mobile_behaviour.csv'.\n",
      "Sample of 'DATE_ID' after transformation:\n",
      "0    2024-12-01\n",
      "1    2024-12-01\n",
      "2    2024-12-01\n",
      "3    2024-12-01\n",
      "4    2024-12-01\n",
      "Name: DATE_ID, dtype: object\n",
      "Transformed file saved to: '../experiments/clean_data/file_access_mobile_behaviour.csv'.\n",
      "\n",
      "All targeted CSV file transformations completed.\n",
      "\n",
      "After this script runs, you can use the CSVs in:\n",
      "'../experiments/clean_data/' for re-uploading to BigQuery.\n",
      "These files will have DATE_ID in YYYY-MM-DD format and all column names in CAPS.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tempfile # Although not used for the final output, good for temporary operations\n",
    "\n",
    "def transform_and_save_csv_dates(input_directory: str, output_directory: str):\n",
    "    \"\"\"\n",
    "    Reads CSV files ending with '_mobile_behaviour.csv' from the input directory,\n",
    "    transforms the 'DATE_ID' column from 'D/M/YYYY' or 'DD/MM/YYYY' to 'YYYY-MM-DD' format,\n",
    "    converts all column names to uppercase, and saves the processed files\n",
    "    to the specified output directory.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): The path to the directory containing the original CSV files.\n",
    "        output_directory (str): The path to the directory where transformed CSV files will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_directory):\n",
    "        print(f\"Error: Input directory '{input_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True) # Create output directory if it doesn't exist\n",
    "\n",
    "    print(f\"Searching for '_mobile_behaviour.csv' files in: {input_directory}\")\n",
    "\n",
    "    csv_files = [f for f in os.listdir(input_directory) if f.endswith('_mobile_behaviour.csv')]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No '_mobile_behaviour.csv' files found in '{input_directory}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} files to process: {', '.join(csv_files)}\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        full_input_path = os.path.join(input_directory, csv_file)\n",
    "        full_output_path = os.path.join(output_directory, csv_file)\n",
    "\n",
    "        print(f\"\\n--- Processing file: '{csv_file}' ---\")\n",
    "        try:\n",
    "            # Read CSV into a pandas DataFrame\n",
    "            df = pd.read_csv(full_input_path)\n",
    "            print(f\"Successfully read '{csv_file}'. Original rows: {len(df)}\")\n",
    "\n",
    "            # Convert all column names to uppercase\n",
    "            df.columns = df.columns.str.upper()\n",
    "            print(\"Column names converted to uppercase.\")\n",
    "\n",
    "            # Attempt to transform 'DATE_ID' column\n",
    "            if 'DATE_ID' in df.columns:\n",
    "                print(f\"Transforming 'DATE_ID' column...\")\n",
    "                print(f\"Sample of 'DATE_ID' before transformation:\\n{df['DATE_ID'].head()}\")\n",
    "                try:\n",
    "                    # Try parsing with '%#d/%#m/%Y' (flexible D/M/YYYY) based on new sample data\n",
    "                    df['DATE_ID'] = pd.to_datetime(df['DATE_ID'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "                    # Fallback to other common formats if the primary parse results in all NaT\n",
    "                    if df['DATE_ID'].isnull().all():\n",
    "                        print(f\"Warning: Primary parsing ('%#d/%#m/%Y') resulted in all empty values. Attempting other common formats for '{csv_file}'.\")\n",
    "                        # Try YYYY-DD-MM (original request)\n",
    "                        df['DATE_ID'] = pd.to_datetime(df['DATE_ID'], format='%Y-%d-%m', errors='coerce')\n",
    "                        if df['DATE_ID'].isnull().all():\n",
    "                            # Try YYYY-MM-DD (another common format)\n",
    "                            df['DATE_ID'] = pd.to_datetime(df['DATE_ID'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "                    # Convert valid datetime objects back to 'YYYY-MM-DD' string format\n",
    "                    # This will result in 'NaN' for any dates that couldn't be parsed by any format\n",
    "                    df['DATE_ID'] = df['DATE_ID'].dt.strftime('%Y-%m-%d')\n",
    "                    print(f\"Successfully transformed 'DATE_ID' column in '{csv_file}'.\")\n",
    "                    print(f\"Sample of 'DATE_ID' after transformation:\\n{df['DATE_ID'].head()}\")\n",
    "\n",
    "                    # Log how many values became NaN\n",
    "                    nan_count = df['DATE_ID'].isnull().sum()\n",
    "                    if nan_count > 0:\n",
    "                        print(f\"Note: {nan_count} rows in 'DATE_ID' column became NaN after transformation, indicating unparsable original values.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not transform 'DATE_ID' column in '{csv_file}': {e}\")\n",
    "                    print(\"This might happen if some date values are not in a parsable format.\")\n",
    "            else:\n",
    "                print(f\"Warning: 'DATE_ID' column not found in '{csv_file}'. Skipping date transformation for this file.\")\n",
    "\n",
    "            # Save the transformed DataFrame to the output directory\n",
    "            df.to_csv(full_output_path, index=False, encoding='utf-8')\n",
    "            print(f\"Transformed file saved to: '{full_output_path}'.\")\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: '{csv_file}' is empty. Skipping.\")\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"Error parsing '{csv_file}': {e}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing '{csv_file}': {e}\")\n",
    "\n",
    "    print(\"\\nAll targeted CSV file transformations completed.\")\n",
    "\n",
    "\n",
    "INPUT_CSV_DIRECTORY = \"../data\"\n",
    "# Define a new directory to save the fixed CSVs\n",
    "OUTPUT_CSV_DIRECTORY = \"../experiments/clean_data/\" # Or any other desired path\n",
    "\n",
    "# Call the function to perform the transformation and saving\n",
    "transform_and_save_csv_dates(INPUT_CSV_DIRECTORY, OUTPUT_CSV_DIRECTORY)\n",
    "\n",
    "print(\"\\nAfter this script runs, you can use the CSVs in:\")\n",
    "print(f\"'{OUTPUT_CSV_DIRECTORY}' for re-uploading to BigQuery.\")\n",
    "print(\"These files will have DATE_ID in YYYY-MM-DD format and all column names in CAPS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "736f08a5-aeeb-430c-abce-0464bfdea055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.api_core import exceptions\n",
    "import tempfile\n",
    "\n",
    "# --- Configuration ---\n",
    "# Replace with your Google Cloud Project ID and BigQuery Dataset ID\n",
    "PROJECT_ID = \"cymbal-telco-da\"\n",
    "DATASET_ID = \"cymbal_telco_dataset\"\n",
    "CSV_DIRECTORY = \"../experiments/clean_data/\" # Path to your CSV files\n",
    "TABLE_DESCRIPTIONS_FILE = \"../experiments/clean_data/table_descriptions.json\" # Path to your table descriptions JSON\n",
    "COLUMN_DESCRIPTIONS_FILE = \"../experiments/clean_data/column_descriptions.json\" # Path to your column descriptions JSON\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "dataset_ref = client.dataset(DATASET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eead814-b1db-4cd5-a0fc-07be5c7257c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset_if_not_exists(client, dataset_id, project_id):\n",
    "    \"\"\"\n",
    "    Checks if a BigQuery dataset exists and creates it if not.\n",
    "\n",
    "    Args:\n",
    "        client (google.cloud.bigquery.Client): The BigQuery client.\n",
    "        dataset_id (str): The ID of the BigQuery dataset.\n",
    "        project_id (str): The Google Cloud Project ID.\n",
    "    \"\"\"\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    try:\n",
    "        client.get_dataset(dataset_ref)\n",
    "        print(f\"Dataset '{dataset_id}' already exists in project '{project_id}'.\")\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"Dataset '{dataset_id}' not found. Creating dataset...\")\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = \"US\"  # Set the location for your dataset (e.g., \"US\", \"EU\", \"ASIA-SOUTHEAST1\")\n",
    "        try:\n",
    "            dataset = client.create_dataset(dataset, timeout=30)\n",
    "            print(f\"Dataset '{dataset_id}' created successfully in project '{project_id}'.\")\n",
    "        except exceptions.Conflict:\n",
    "            print(f\"Dataset '{dataset_id}' already exists (was created concurrently).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating dataset '{dataset_id}': {e}\")\n",
    "\n",
    "def upload_csv_to_bigquery_with_caps_columns(csv_file_path, table_id, client, dataset_ref):\n",
    "    \"\"\"\n",
    "    Uploads a CSV file to a BigQuery table, ensuring all column names are in CAPS.\n",
    "    A new table will be created if it doesn't exist. The schema will be auto-detected.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): The full path to the CSV file.\n",
    "        table_id (str): The ID of the BigQuery table to load data into.\n",
    "        client (google.cloud.bigquery.Client): The BigQuery client.\n",
    "        dataset_ref (google.cloud.bigquery.DatasetReference): Reference to the BigQuery dataset.\n",
    "    \"\"\"\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    temp_csv_path = None # Initialize to None\n",
    "\n",
    "    try:\n",
    "        # Read CSV into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Convert all column names to uppercase\n",
    "        df.columns = df.columns.str.upper()\n",
    "        print(f\"Converted column names to uppercase for {os.path.basename(csv_file_path)}.\")\n",
    "\n",
    "        # Save the modified DataFrame to a temporary CSV file\n",
    "        # Use tempfile for secure temporary file creation and automatic cleanup\n",
    "        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.csv', encoding='utf-8') as tmp_csv:\n",
    "            df.to_csv(tmp_csv.name, index=False, encoding='utf-8')\n",
    "            temp_csv_path = tmp_csv.name\n",
    "        print(f\"Saved temporary CSV with uppercase columns: {temp_csv_path}\")\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            skip_leading_rows=1,  # Skip header row (the new uppercase one)\n",
    "            autodetect=True,      # Auto-detect schema from CSV\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Overwrite if table exists\n",
    "        )\n",
    "\n",
    "        with open(temp_csv_path, \"rb\") as source_file:\n",
    "            load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "        print(f\"Starting job {load_job.job_id} for table {table_id} from {csv_file_path}\")\n",
    "        load_job.result() # Waits for the job to complete\n",
    "        print(f\"Successfully loaded {load_job.output_rows} rows into {table_id}.\")\n",
    "\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"Dataset {DATASET_ID} not found. Please ensure it exists in project {PROJECT_ID}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {csv_file_path} to {table_id}: {e}\")\n",
    "    finally:\n",
    "        # Clean up the temporary CSV file\n",
    "        if temp_csv_path and os.path.exists(temp_csv_path):\n",
    "            os.remove(temp_csv_path)\n",
    "            print(f\"Cleaned up temporary CSV: {temp_csv_path}\")\n",
    "\n",
    "\n",
    "def update_table_descriptions(bq_table_id, descriptions_file, client, dataset_ref):\n",
    "    \"\"\"\n",
    "    Updates the descriptions for a specific BigQuery table based on a JSON file.\n",
    "\n",
    "    Args:\n",
    "        bq_table_id (str): The ID of the BigQuery table to update.\n",
    "        descriptions_file (str): The path to the JSON file containing table descriptions.\n",
    "        client (google.cloud.bigquery.Client): The BigQuery client.\n",
    "        dataset_ref (google.cloud.bigquery.DatasetReference): Reference to the BigQuery dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(descriptions_file):\n",
    "        print(f\"Table descriptions file not found: {descriptions_file}\")\n",
    "        return\n",
    "\n",
    "    with open(descriptions_file, 'r') as f:\n",
    "        table_descriptions_data = json.load(f)\n",
    "\n",
    "    found_description = False\n",
    "    for item in table_descriptions_data:\n",
    "        # Try to match both possible keys from the JSON, normalizing to uppercase for robust matching\n",
    "        json_table_name_1 = item.get(\"Data Source Name\")\n",
    "        json_table_name_2 = item.get(\"table_name\")\n",
    "        description = item.get(\"description\")\n",
    "\n",
    "        # Check if the current bq_table_id matches any of the names in the JSON entry (case-insensitive)\n",
    "        if (json_table_name_1 and json_table_name_1.upper() == bq_table_id.upper()) or \\\n",
    "           (json_table_name_2 and json_table_name_2.upper() == bq_table_id.upper()):\n",
    "            found_description = True\n",
    "            if not description:\n",
    "                print(f\"No description found in JSON for table {bq_table_id}. Skipping description update for this table.\")\n",
    "                break # Found the entry, but no description to apply\n",
    "\n",
    "            try:\n",
    "                table = client.get_table(dataset_ref.table(bq_table_id)) # Fetch existing table\n",
    "                table.description = description # Update description\n",
    "                table = client.update_table(table, [\"description\"]) # Send API request\n",
    "                print(f\"Updated description for table: {bq_table_id}\")\n",
    "            except exceptions.NotFound:\n",
    "                print(f\"Table {bq_table_id} not found in dataset {DATASET_ID}. Skipping description update.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating description for table {bq_table_id}: {e}\")\n",
    "            break # Found and processed the description for this table, exit loop\n",
    "\n",
    "    if not found_description:\n",
    "        print(f\"No description found in {descriptions_file} for table: {bq_table_id}. Skipping table description update.\")\n",
    "\n",
    "\n",
    "def update_column_descriptions(table_id, descriptions_file, client, dataset_ref):\n",
    "    \"\"\"\n",
    "    Updates the descriptions for BigQuery table columns based on a JSON file,\n",
    "    performing case-insensitive matching for column names and normalizing column names\n",
    "    to uppercase during lookup.\n",
    "\n",
    "    Args:\n",
    "        table_id (str): The ID of the BigQuery table whose columns are to be updated.\n",
    "        descriptions_file (str): The path to the JSON file containing column descriptions.\n",
    "        client (google.cloud.bigquery.Client): The BigQuery client.\n",
    "        dataset_ref (google.cloud.bigquery.DatasetReference): Reference to the BigQuery dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(descriptions_file):\n",
    "        print(f\"Column descriptions file not found: {descriptions_file}\")\n",
    "        return\n",
    "\n",
    "    with open(descriptions_file, 'r') as f:\n",
    "        raw_all_column_descriptions = json.load(f)\n",
    "\n",
    "    # Normalize all table and column description keys in the loaded JSON to uppercase\n",
    "    normalized_all_column_descriptions = {}\n",
    "    for json_table_name, cols_data in raw_all_column_descriptions.items():\n",
    "        normalized_cols = {col_key.upper(): col_value for col_key, col_value in cols_data.items()}\n",
    "        normalized_all_column_descriptions[json_table_name.upper()] = normalized_cols\n",
    "    print(f\"Loaded and normalized column descriptions from '{descriptions_file}'.\")\n",
    "\n",
    "    # Check if we have descriptions for the specific table being processed\n",
    "    if table_id.upper() not in normalized_all_column_descriptions:\n",
    "        print(f\"No column descriptions found in JSON for table '{table_id}'. Skipping column description update for this table.\")\n",
    "        return\n",
    "\n",
    "    columns_data_from_json = normalized_all_column_descriptions[table_id.upper()]\n",
    "\n",
    "    try:\n",
    "        table_ref = dataset_ref.table(table_id)\n",
    "        table = client.get_table(table_ref)  # Fetch current table metadata\n",
    "\n",
    "        original_schema = table.schema\n",
    "        new_schema = []\n",
    "        updated_count = 0\n",
    "\n",
    "        for field in original_schema:\n",
    "            # Match field.name (from BigQuery table) with keys in columns_data_from_json (from JSON)\n",
    "            # by converting field.name to uppercase for lookup.\n",
    "            column_description_data = columns_data_from_json.get(field.name.upper())\n",
    "\n",
    "            if column_description_data:\n",
    "                new_description = column_description_data.get(\"description\")\n",
    "\n",
    "                if new_description and new_description != field.description:\n",
    "                    new_field = bigquery.SchemaField(\n",
    "                        name=field.name,  # Keep original casing for the field name in the schema (which should be CAPS now)\n",
    "                        field_type=field.field_type,\n",
    "                        mode=field.mode,\n",
    "                        description=new_description,\n",
    "                        fields=field.fields  # Preserve nested fields if any\n",
    "                    )\n",
    "                    new_schema.append(new_field)\n",
    "                    updated_count += 1\n",
    "                    print(f\"  - Updated description for column '{field.name}'.\")\n",
    "                else:\n",
    "                    new_schema.append(field) # No change needed or description is already identical\n",
    "            else:\n",
    "                new_schema.append(field) # Keep original field if no new description found for it\n",
    "\n",
    "        if updated_count > 0:\n",
    "            # Update the table with the new schema (which includes updated descriptions)\n",
    "            table.schema = new_schema\n",
    "            table = client.update_table(table, [\"schema\"])\n",
    "            print(f\"Successfully updated {updated_count} column descriptions for table: {table_id}\")\n",
    "        else:\n",
    "            print(f\"No column descriptions needed updating for table: {table_id}.\")\n",
    "\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"Table '{table_id}' not found in dataset '{DATASET_ID}'. Skipping column description update for this table.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating column descriptions for table '{table_id}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b68902ab-a314-4a96-84e5-eb5825b0ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured for Project: cymbal-telco-da, Dataset: cymbal_telco_dataset\n",
      "Dataset 'cymbal_telco_dataset' already exists in project 'cymbal-telco-da'.\n",
      "BigQuery client initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Flow ---\n",
    "\n",
    "# Cell 1: Setup and Configuration\n",
    "print(f\"Configured for Project: {PROJECT_ID}, Dataset: {DATASET_ID}\")\n",
    "create_dataset_if_not_exists(client, DATASET_ID, PROJECT_ID)\n",
    "print(\"BigQuery client initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d310363-ca6b-4c77-97de-5dc5a83650e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Available CSV files in clean_data/ ---\n",
      "gaming_mobile_behaviour.csv\n",
      "streaming_mobile_behaviour.csv\n",
      "im_mobile_behaviour.csv\n",
      "voip_mobile_behaviour.csv\n",
      "other_mobile_behaviour.csv\n",
      "browsing_mobile_behaviour.csv\n",
      "file_access_mobile_behaviour.csv\n",
      "\n",
      "--- Processing browsing_mobile_behaviour.csv (Intended BigQuery Table ID: browsing_mobile_behaviour) ---\n",
      "\n",
      "--- Uploading CSV Data to BigQuery for table browsing_mobile_behaviour ---\n",
      "Converted column names to uppercase for browsing_mobile_behaviour.csv.\n",
      "Saved temporary CSV with uppercase columns: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpgig228_z.csv\n",
      "Starting job 04091eb7-8018-4eb8-8b2d-53c97d501b63 for table browsing_mobile_behaviour from ../experiments/clean_data/browsing_mobile_behaviour.csv\n",
      "Successfully loaded 652926 rows into browsing_mobile_behaviour.\n",
      "Cleaned up temporary CSV: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpgig228_z.csv\n",
      "\n",
      "--- Updating BigQuery Table Description for table browsing_mobile_behaviour ---\n",
      "No description found in ../experiments/clean_data/table_descriptions.json for table: browsing_mobile_behaviour. Skipping table description update.\n",
      "\n",
      "--- Updating BigQuery Column Descriptions for table browsing_mobile_behaviour ---\n",
      "Loaded and normalized column descriptions from '../experiments/clean_data/column_descriptions.json'.\n",
      "  - Updated description for column 'DATE_ID'.\n",
      "  - Updated description for column 'RAT'.\n",
      "  - Updated description for column 'APP_NAME'.\n",
      "  - Updated description for column 'CELL_NAME'.\n",
      "  - Updated description for column 'PROTOCOL'.\n",
      "  - Updated description for column 'HOST'.\n",
      "  - Updated description for column 'TOTAL_EVENT_DURATION_S'.\n",
      "  - Updated description for column 'TOTAL_DL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'TOTAL_UL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'MSISDN_MASKED'.\n",
      "Successfully updated 10 column descriptions for table: browsing_mobile_behaviour\n",
      "\n",
      "--- Processing file_access_mobile_behaviour.csv (Intended BigQuery Table ID: file_access_mobile_behaviour) ---\n",
      "\n",
      "--- Uploading CSV Data to BigQuery for table file_access_mobile_behaviour ---\n",
      "Converted column names to uppercase for file_access_mobile_behaviour.csv.\n",
      "Saved temporary CSV with uppercase columns: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmp63aq7aqp.csv\n",
      "Starting job ba85560d-69af-4cc5-bec8-6dbea7adc42a for table file_access_mobile_behaviour from ../experiments/clean_data/file_access_mobile_behaviour.csv\n",
      "Successfully loaded 331669 rows into file_access_mobile_behaviour.\n",
      "Cleaned up temporary CSV: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmp63aq7aqp.csv\n",
      "\n",
      "--- Updating BigQuery Table Description for table file_access_mobile_behaviour ---\n",
      "Updated description for table: file_access_mobile_behaviour\n",
      "\n",
      "--- Updating BigQuery Column Descriptions for table file_access_mobile_behaviour ---\n",
      "Loaded and normalized column descriptions from '../experiments/clean_data/column_descriptions.json'.\n",
      "  - Updated description for column 'DATE_ID'.\n",
      "  - Updated description for column 'RAT'.\n",
      "  - Updated description for column 'APP_NAME'.\n",
      "  - Updated description for column 'CELL_NAME'.\n",
      "  - Updated description for column 'PROTOCOL'.\n",
      "  - Updated description for column 'HOST'.\n",
      "  - Updated description for column 'TOTAL_EVENT_DURATION_S'.\n",
      "  - Updated description for column 'TOTAL_DL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'TOTAL_UL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'MSISDN_MASKED'.\n",
      "Successfully updated 10 column descriptions for table: file_access_mobile_behaviour\n",
      "\n",
      "--- Processing gaming_mobile_behaviour.csv (Intended BigQuery Table ID: gaming_mobile_behaviour) ---\n",
      "\n",
      "--- Uploading CSV Data to BigQuery for table gaming_mobile_behaviour ---\n",
      "Converted column names to uppercase for gaming_mobile_behaviour.csv.\n",
      "Saved temporary CSV with uppercase columns: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpj_gh5pe4.csv\n",
      "Starting job b687694c-6998-46f1-9e07-ed653d76cf7b for table gaming_mobile_behaviour from ../experiments/clean_data/gaming_mobile_behaviour.csv\n",
      "Successfully loaded 8671 rows into gaming_mobile_behaviour.\n",
      "Cleaned up temporary CSV: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpj_gh5pe4.csv\n",
      "\n",
      "--- Updating BigQuery Table Description for table gaming_mobile_behaviour ---\n",
      "Updated description for table: gaming_mobile_behaviour\n",
      "\n",
      "--- Updating BigQuery Column Descriptions for table gaming_mobile_behaviour ---\n",
      "Loaded and normalized column descriptions from '../experiments/clean_data/column_descriptions.json'.\n",
      "  - Updated description for column 'DATE_ID'.\n",
      "  - Updated description for column 'RAT'.\n",
      "  - Updated description for column 'APP_NAME'.\n",
      "  - Updated description for column 'CELL_NAME'.\n",
      "  - Updated description for column 'PROTOCOL'.\n",
      "  - Updated description for column 'HOST'.\n",
      "  - Updated description for column 'TOTAL_EVENT_DURATION_S'.\n",
      "  - Updated description for column 'TOTAL_DL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'TOTAL_UL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'MSISDN_MASKED'.\n",
      "Successfully updated 10 column descriptions for table: gaming_mobile_behaviour\n",
      "\n",
      "--- Processing im_mobile_behaviour.csv (Intended BigQuery Table ID: im_mobile_behaviour) ---\n",
      "\n",
      "--- Uploading CSV Data to BigQuery for table im_mobile_behaviour ---\n",
      "Converted column names to uppercase for im_mobile_behaviour.csv.\n",
      "Saved temporary CSV with uppercase columns: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmp01zrei7l.csv\n",
      "Starting job 167b9cb7-badc-45a2-82eb-7e9c35bf2376 for table im_mobile_behaviour from ../experiments/clean_data/im_mobile_behaviour.csv\n",
      "Successfully loaded 115801 rows into im_mobile_behaviour.\n",
      "Cleaned up temporary CSV: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmp01zrei7l.csv\n",
      "\n",
      "--- Updating BigQuery Table Description for table im_mobile_behaviour ---\n",
      "Updated description for table: im_mobile_behaviour\n",
      "\n",
      "--- Updating BigQuery Column Descriptions for table im_mobile_behaviour ---\n",
      "Loaded and normalized column descriptions from '../experiments/clean_data/column_descriptions.json'.\n",
      "  - Updated description for column 'DATE_ID'.\n",
      "  - Updated description for column 'RAT'.\n",
      "  - Updated description for column 'APP_NAME'.\n",
      "  - Updated description for column 'CELL_NAME'.\n",
      "  - Updated description for column 'PROTOCOL'.\n",
      "  - Updated description for column 'HOST'.\n",
      "  - Updated description for column 'TOTAL_EVENT_DURATION_S'.\n",
      "  - Updated description for column 'TOTAL_DL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'TOTAL_UL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'MSISDN_MASKED'.\n",
      "Successfully updated 10 column descriptions for table: im_mobile_behaviour\n",
      "\n",
      "--- Processing other_mobile_behaviour.csv (Intended BigQuery Table ID: other_mobile_behaviour) ---\n",
      "\n",
      "--- Uploading CSV Data to BigQuery for table other_mobile_behaviour ---\n",
      "Converted column names to uppercase for other_mobile_behaviour.csv.\n",
      "Saved temporary CSV with uppercase columns: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpoldn5ei0.csv\n",
      "Starting job 1ca597e2-e4a1-498c-805d-5293b9e9b0e7 for table other_mobile_behaviour from ../experiments/clean_data/other_mobile_behaviour.csv\n",
      "Successfully loaded 529832 rows into other_mobile_behaviour.\n",
      "Cleaned up temporary CSV: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpoldn5ei0.csv\n",
      "\n",
      "--- Updating BigQuery Table Description for table other_mobile_behaviour ---\n",
      "Updated description for table: other_mobile_behaviour\n",
      "\n",
      "--- Updating BigQuery Column Descriptions for table other_mobile_behaviour ---\n",
      "Loaded and normalized column descriptions from '../experiments/clean_data/column_descriptions.json'.\n",
      "  - Updated description for column 'DATE_ID'.\n",
      "  - Updated description for column 'RAT'.\n",
      "  - Updated description for column 'APP_NAME'.\n",
      "  - Updated description for column 'CELL_NAME'.\n",
      "  - Updated description for column 'PROTOCOL'.\n",
      "  - Updated description for column 'HOST'.\n",
      "  - Updated description for column 'TOTAL_EVENT_DURATION_S'.\n",
      "  - Updated description for column 'TOTAL_DL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'TOTAL_UL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'MSISDN_MASKED'.\n",
      "Successfully updated 10 column descriptions for table: other_mobile_behaviour\n",
      "\n",
      "--- Processing streaming_mobile_behaviour.csv (Intended BigQuery Table ID: streaming_mobile_behaviour) ---\n",
      "\n",
      "--- Uploading CSV Data to BigQuery for table streaming_mobile_behaviour ---\n",
      "Converted column names to uppercase for streaming_mobile_behaviour.csv.\n",
      "Saved temporary CSV with uppercase columns: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpgqrn1n0t.csv\n",
      "Starting job b87b6792-3aed-4771-9a09-439edc08873f for table streaming_mobile_behaviour from ../experiments/clean_data/streaming_mobile_behaviour.csv\n",
      "Successfully loaded 85620 rows into streaming_mobile_behaviour.\n",
      "Cleaned up temporary CSV: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpgqrn1n0t.csv\n",
      "\n",
      "--- Updating BigQuery Table Description for table streaming_mobile_behaviour ---\n",
      "Updated description for table: streaming_mobile_behaviour\n",
      "\n",
      "--- Updating BigQuery Column Descriptions for table streaming_mobile_behaviour ---\n",
      "Loaded and normalized column descriptions from '../experiments/clean_data/column_descriptions.json'.\n",
      "  - Updated description for column 'DATE_ID'.\n",
      "  - Updated description for column 'RAT'.\n",
      "  - Updated description for column 'APP_NAME'.\n",
      "  - Updated description for column 'CELL_NAME'.\n",
      "  - Updated description for column 'PROTOCOL'.\n",
      "  - Updated description for column 'HOST'.\n",
      "  - Updated description for column 'TOTAL_EVENT_DURATION_S'.\n",
      "  - Updated description for column 'TOTAL_DL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'TOTAL_UL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'MSISDN_MASKED'.\n",
      "Successfully updated 10 column descriptions for table: streaming_mobile_behaviour\n",
      "\n",
      "--- Processing voip_mobile_behaviour.csv (Intended BigQuery Table ID: voip_mobile_behaviour) ---\n",
      "\n",
      "--- Uploading CSV Data to BigQuery for table voip_mobile_behaviour ---\n",
      "Converted column names to uppercase for voip_mobile_behaviour.csv.\n",
      "Saved temporary CSV with uppercase columns: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpem1e5fou.csv\n",
      "Starting job 9180979a-9906-426d-a8e0-6c8c2b069dee for table voip_mobile_behaviour from ../experiments/clean_data/voip_mobile_behaviour.csv\n",
      "Successfully loaded 11933 rows into voip_mobile_behaviour.\n",
      "Cleaned up temporary CSV: /var/folders/88/pldbvcsj07lgrpgmryy9w43r01b183/T/tmpem1e5fou.csv\n",
      "\n",
      "--- Updating BigQuery Table Description for table voip_mobile_behaviour ---\n",
      "Updated description for table: voip_mobile_behaviour\n",
      "\n",
      "--- Updating BigQuery Column Descriptions for table voip_mobile_behaviour ---\n",
      "Loaded and normalized column descriptions from '../experiments/clean_data/column_descriptions.json'.\n",
      "  - Updated description for column 'DATE_ID'.\n",
      "  - Updated description for column 'RAT'.\n",
      "  - Updated description for column 'APP_NAME'.\n",
      "  - Updated description for column 'CELL_NAME'.\n",
      "  - Updated description for column 'PROTOCOL'.\n",
      "  - Updated description for column 'HOST'.\n",
      "  - Updated description for column 'TOTAL_EVENT_DURATION_S'.\n",
      "  - Updated description for column 'TOTAL_DL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'TOTAL_UL_TRAFFIC_BYTES'.\n",
      "  - Updated description for column 'MSISDN_MASKED'.\n",
      "Successfully updated 10 column descriptions for table: voip_mobile_behaviour\n",
      "\n",
      "All BigQuery upload and description update processes complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: List CSV files (for confirmation, not strictly necessary for the script but useful for debugging)\n",
    "print(\"\\n--- Available CSV files in clean_data/ ---\")\n",
    "csv_files_in_dir = [f for f in os.listdir('../experiments/clean_data/') if f.endswith('_mobile_behaviour.csv')]\n",
    "for f in csv_files_in_dir:\n",
    "    print(f)\n",
    "\n",
    "# Define a mapping for file names to BigQuery table IDs if they differ from the filename base.\n",
    "# These table IDs will be used as the BigQuery table names.\n",
    "bq_table_name_map = {\n",
    "    \"subscribers_info.csv\": \"subscribers_info\",\n",
    "    \"fibre_connected_devices_info.csv\": \"fibre_connected_devices_info\",\n",
    "    \"fibre_behaviour.csv\": \"fibre_behaviour\",\n",
    "    \"browsing_mobile_behaviour.csv\": \"browsing_mobile_behaviour\", # Corrected original typo \"broswing\" to \"browsing\"\n",
    "    \"im_mobile_behaviour.csv\": \"im_mobile_behaviour\",\n",
    "    \"other_mobile_behaviour.csv\": \"other_mobile_behaviour\",\n",
    "    \"streaming_mobile_behaviour.csv\": \"streaming_mobile_behaviour\",\n",
    "    \"file_access_mobile_behaviour.csv\": \"file_access_mobile_behaviour\",\n",
    "    \"gaming_mobile_behaviour.csv\": \"gaming_mobile_behaviour\",\n",
    "    \"voip_mobile_behaviour.csv\": \"voip_mobile_behaviour\"\n",
    "}\n",
    "\n",
    "# Sort csv files for consistent processing order (optional but good for reproducibility)\n",
    "csv_files_in_dir.sort()\n",
    "\n",
    "for csv_file in csv_files_in_dir:\n",
    "    # Get the base filename (e.g., \"subscribers_info.csv\")\n",
    "    base_file_name = csv_file\n",
    "    # Get the BigQuery table ID from the map, or derive from filename (without .csv extension)\n",
    "    table_id = bq_table_name_map.get(base_file_name, os.path.splitext(base_file_name)[0])\n",
    "\n",
    "    csv_full_path = os.path.join(CSV_DIRECTORY, csv_file)\n",
    "\n",
    "    print(f\"\\n--- Processing {csv_file} (Intended BigQuery Table ID: {table_id}) ---\")\n",
    "\n",
    "    # --- Step 1: Upload CSV Data to BigQuery with CAPS columns ---\n",
    "    print(f\"\\n--- Uploading CSV Data to BigQuery for table {table_id} ---\")\n",
    "    upload_csv_to_bigquery_with_caps_columns(csv_full_path, table_id, client, dataset_ref)\n",
    "\n",
    "    # --- Step 2: Update BigQuery Table Descriptions ---\n",
    "    print(f\"\\n--- Updating BigQuery Table Description for table {table_id} ---\")\n",
    "    update_table_descriptions(table_id, TABLE_DESCRIPTIONS_FILE, client, dataset_ref)\n",
    "\n",
    "    # --- Step 3: Update BigQuery Column Descriptions ---\n",
    "    print(f\"\\n--- Updating BigQuery Column Descriptions for table {table_id} ---\")\n",
    "    update_column_descriptions(table_id, COLUMN_DESCRIPTIONS_FILE, client, dataset_ref)\n",
    "\n",
    "print(\"\\nAll BigQuery upload and description update processes complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770bf2c-591f-41d2-b367-c9f0d7254cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7a469-4a68-4fe1-b3ae-e653ac2aefc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
